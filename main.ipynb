{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8949bc18",
   "metadata": {},
   "source": [
    "## Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52684bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, io\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from IPython.display import clear_output\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbd9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ridership_df = pd.DataFrame()\n",
    "\n",
    "links = [\n",
    "    'https://www.bart.gov/sites/default/files/2025-11/Ridership_202510.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-10/Ridership_202509.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-10/Ridership_202508.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-08/Ridership_202507.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-07/Ridership_202506.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-06/Ridership_202505.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-05/Ridership_202504.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-05/Ridership_202503.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-03/Ridership_202502.xlsx',\n",
    "    'https://www.bart.gov/sites/default/files/2025-02/Ridership_202501.xlsx'\n",
    "]\n",
    "\n",
    "def extract_date_from_link(link):\n",
    "    \"\"\"\n",
    "    Extract substring between 'Ridership_' and '.xlsx'.\n",
    "    Example: 'Ridership_202510.xlsx' → '202510'\n",
    "    \"\"\"\n",
    "    start = link.find(\"Ridership_\") + len(\"Ridership_\")\n",
    "    end = link.find(\".xlsx\")\n",
    "    return link[start:end]\n",
    "\n",
    "def set_header_row(df, keyword=\"Exit Station Two-Letter Code\"):\n",
    "    \"\"\"\n",
    "    Find the row containing keyword and use it as the column headers.\n",
    "    Return the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # locate the header row index\n",
    "    header_idx = df.index[df.apply(lambda row: row.astype(str).str.contains(keyword).any(), axis=1)]\n",
    "    \n",
    "    if len(header_idx) == 0:\n",
    "        # no header row found — return unchanged\n",
    "        return df\n",
    "    \n",
    "    header_idx = header_idx[0]\n",
    "\n",
    "    # set the header\n",
    "    new_header = df.iloc[header_idx]\n",
    "    df = df[header_idx + 1 :]  # drop the header row and all above it\n",
    "    df.columns = new_header\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    response = requests.get(link)\n",
    "\n",
    "    df = pd.read_excel(\n",
    "        io.BytesIO(response.content),\n",
    "        engine='openpyxl',\n",
    "        header=None,                    \n",
    "        sheet_name='Average Weekday'\n",
    "    )\n",
    "\n",
    "    df = set_header_row(df)\n",
    "\n",
    "    # Add extracted date\n",
    "    df[\"date\"] = extract_date_from_link(link)            \n",
    "\n",
    "    ridership_df = pd.concat([ridership_df, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2334d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bart.gov/sites/default/files/docs/station-names.xls\n",
    "\n",
    "station_map_df = pd.DataFrame({\n",
    "    \"code\": [\n",
    "        \"RM\",\"EN\",\"EP\",\"NB\",\"BK\",\"AS\",\"MA\",\"19\",\"12\",\"LM\",\"FV\",\"CL\",\"SL\",\"BF\",\"HY\",\"SH\",\n",
    "        \"UC\",\"FM\",\"CN\",\"PH\",\"WC\",\"LF\",\"OR\",\"RR\",\"OW\",\"EM\",\"MT\",\"PL\",\"CC\",\"16\",\"24\",\"GP\",\n",
    "        \"BP\",\"DC\",\"CM\",\"CV\",\"ED\",\"NC\",\"WP\",\"SS\",\"SB\",\"SO\",\"MB\",\"WD\",\"OA\",\"WS\",\"AN\",\"PC\",\n",
    "        \"ML\",\"BE\"\n",
    "    ],\n",
    "    \"stop_name\": [\n",
    "        \"Richmond\",\"El Cerrito Del Norte\",\"El Cerrito Plaza\",\"North Berkeley\",\n",
    "        \"Downtown Berkeley\",\"Ashby\",\"MacArthur\",\"19th Street Oakland\",\n",
    "        \"12th Street / Oakland City Center\",\"Lake Merritt\",\"Fruitvale\",\"Coliseum - OAC\",\n",
    "        \"San Leandro\",\"Bay Fair\",\"Hayward\",\"South Hayward\",\"Union City\",\"Fremont\",\n",
    "        \"Concord\",\"Pleasant Hill / Contra Costa Centre\",\"Walnut Creek\",\"Lafayette\",\"Orinda\",\"Rockridge\",\n",
    "        \"West Oakland\",\"Embarcadero\",\"Montgomery Street\",\"Powell Street\",\n",
    "        \"Civic Center / UN Plaza\",\"16th Street / Mission\",\"24th Street / Mission\",\"Glen Park\",\n",
    "        \"Balboa Park\",\"Daly City\",\"Colma\",\"Castro Valley\",\"Dublin / Pleasanton\",\n",
    "        \"North Concord / Martinez\",\"Pittsburg / Bay Point\",\"South San Francisco\",\"San Bruno\",\n",
    "        \"San Francisco International Airport\",\"Millbrae (Caltrain Transfer Platform)\",\"West Dublin / Pleasanton\",\n",
    "        \"Oakland International Airport Station\",\"Warm Springs / South Fremont\",\"Antioch\",\"Pittsburg Center\",\n",
    "        \"Milpitas\",\"Berryessa / North San Jose\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f645f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS_URL = \"https://www.bart.gov/dev/schedules/google_transit.zip\"\n",
    "\n",
    "response = requests.get(GTFS_URL)\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Load GTFS components into DataFrames\n",
    "routes = pd.read_csv(z.open(\"routes.txt\"))\n",
    "stops = pd.read_csv(z.open(\"stops.txt\"))\n",
    "trips = pd.read_csv(z.open(\"trips.txt\"), dtype={\"route_id\": str})\n",
    "stop_times = pd.read_csv(z.open(\"stop_times.txt\"))\n",
    "calendar = pd.read_csv(z.open(\"calendar.txt\"))\n",
    "\n",
    "# Join trips with routes\n",
    "trip_routes = trips.merge(routes, on=\"route_id\", how=\"left\")\n",
    "\n",
    "# Join stop_times with stops\n",
    "times_with_stops = stop_times.merge(stops, on=\"stop_id\", how=\"left\")\n",
    "\n",
    "# Final joined table\n",
    "full_schedule = (\n",
    "    trip_routes\n",
    "    .merge(times_with_stops, on=\"trip_id\", how=\"left\")\n",
    "    .sort_values([\"route_id\", \"trip_id\", \"stop_sequence\"])\n",
    ")\n",
    "\n",
    "full_schedule['stop_name'] = full_schedule['stop_name'].str.replace(\"\", \"\", regex=False)\n",
    "\n",
    "mapped_schedule = full_schedule.merge(station_map_df, left_on=\"stop_name\", right_on=\"stop_name\", how=\"left\")\n",
    "\n",
    "# mapped_schedule.to_excel(\"C:/Users/chhri/Downloads/bart_full_schedule.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92409b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ridership_df.rename(columns={'Exit Station Two-Letter Code': 'from_stop'})\n",
    "\n",
    "# 2. Melt all OD columns into long form\n",
    "df_long = df.melt(\n",
    "    id_vars=['date', 'from_stop'],     # keep these as identifiers\n",
    "    var_name='to_stop',                # new column representing destination\n",
    "    value_name='ridership'             # ridership value\n",
    ")\n",
    "\n",
    "# 3. Drop rows where to_stop is NaN (your last row has a NaN column)\n",
    "df_long = df_long.dropna(subset=['to_stop'])\n",
    "\n",
    "# Optional: sort nicely\n",
    "df_long = df_long.sort_values(['date', 'from_stop', 'to_stop']).reset_index(drop=True)\n",
    "\n",
    "df_avg = (\n",
    "    df_long\n",
    "    .groupby(['from_stop', 'to_stop'], as_index=False)['ridership']\n",
    "    .mean()\n",
    "    .rename(columns={'ridership': 'avg_ridership'})\n",
    ")\n",
    "\n",
    "# Round up avg ridership to nearest integer and handle missing values\n",
    "df_avg['avg_ridership'] = df_avg['avg_ridership'].apply(lambda x: int(math.ceil(x)) if pd.notna(x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe227e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df = mapped_schedule.copy()\n",
    "\n",
    "def parse_gtfs_time(t):\n",
    "    \"\"\"\n",
    "    Parse time strings like '06:11:00' or '24:05:00' into minutes since midnight.\n",
    "    GTFS allows hours >= 24, meaning times past midnight but same service day.\n",
    "    \"\"\"\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    hh, mm, ss = map(int, str(t).split(\":\"))\n",
    "    return hh * 60 + mm + ss / 60.0   # minutes since midnight (possibly > 24*60)\n",
    "\n",
    "def compute_travel_times(stops_df):\n",
    "    df = stops_df.copy()\n",
    "\n",
    "    # parse times into numeric minutes\n",
    "    df[\"arr_min\"] = df[\"arrival_time\"].apply(parse_gtfs_time)\n",
    "    df[\"dep_min\"] = df[\"departure_time\"].apply(parse_gtfs_time)\n",
    "\n",
    "    # sort properly\n",
    "    df = df.sort_values([\"route_id\", \"trip_id\", \"stop_sequence\"])\n",
    "\n",
    "    arc_rows = []\n",
    "\n",
    "    for (route_id, trip_id), group in df.groupby([\"route_id\", \"trip_id\"]):\n",
    "        group = group.sort_values(\"stop_sequence\")\n",
    "\n",
    "        dep_times = group[\"dep_min\"].values\n",
    "        arr_times = group[\"arr_min\"].values\n",
    "        seq      = group[\"stop_sequence\"].values\n",
    "        codes    = group[\"code\"].values\n",
    "        dep_str  = group[\"departure_time\"].values\n",
    "        arr_str  = group[\"arrival_time\"].values\n",
    "\n",
    "        for i in range(len(group) - 1):\n",
    "            travel = arr_times[i+1] - dep_times[i]   # minutes\n",
    "            travel_min = int(round(travel))\n",
    "\n",
    "            arc_rows.append({\n",
    "                \"route_id\": route_id,\n",
    "                \"trip_id\": trip_id,\n",
    "                \"from_stop\": codes[i],\n",
    "                \"to_stop\": codes[i+1],\n",
    "                \"from_seq\": seq[i],\n",
    "                \"to_seq\": seq[i+1],\n",
    "                \"dep_time_str\": dep_str[i],\n",
    "                \"arr_time_str\": arr_str[i+1],\n",
    "                \"travel_time_min\": travel_min,\n",
    "                \"arr_time\": arr_times[i+1],\n",
    "                \"dep_time\": dep_times[i]\n",
    "            })\n",
    "    df = pd.DataFrame(arc_rows)\n",
    "\n",
    "    arc_rows = df.merge(df_avg, how='left', on=['from_stop','to_stop'])\n",
    "\n",
    "    return pd.DataFrame(arc_rows)\n",
    "\n",
    "arc_df = compute_travel_times(stops_df)\n",
    "\n",
    "arc_df['from_stop_to_stop'] = arc_df['from_stop'] + \"_\" + arc_df['to_stop']\n",
    "\n",
    "arc_df = arc_df[~arc_df.astype(str).apply(lambda row: row.str.contains('nan')).any(axis=1)]\n",
    "\n",
    "arc_df[(arc_df['trip_id'] == 1859829) | (arc_df['trip_id'] == 1859830)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd86cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 12.0.3 build v12.0.3rc0 (win64 - Windows 11.0 (26100.2))\n",
      "\n",
      "CPU model: 13th Gen Intel(R) Core(TM) i7-13700K, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 16 physical cores, 24 logical processors, using up to 24 threads\n",
      "\n",
      "Optimize a model with 117726 rows, 67838 columns and 235450 nonzeros\n",
      "Model fingerprint: 0x63974e29\n",
      "Variable types: 0 continuous, 67838 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 2e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [9e+00, 1e+03]\n",
      "Presolve removed 106426 rows and 42729 columns\n",
      "Presolve time: 0.05s\n",
      "Presolved: 11300 rows, 25109 columns, 50218 nonzeros\n",
      "Variable types: 0 continuous, 25109 integer (0 binary)\n",
      "Performing another presolve...\n",
      "Presolve removed 2461 rows and 5242 columns\n",
      "Presolve time: 0.07s\n",
      "\n",
      "Root relaxation: objective 4.230000e+02, 9958 iterations, 0.20 seconds (0.40 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0     423.0000000  423.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (9958 simplex iterations) in 0.44 seconds (0.68 work units)\n",
      "Thread count was 24 (of 24 available processors)\n",
      "\n",
      "Solution count 1: 423 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.230000000000e+02, best bound 4.230000000000e+02, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "def build_time_expanded_graph(\n",
    "    arc_df,\n",
    "    stop_from_col=\"from_stop\",\n",
    "    stop_to_col=\"to_stop\",\n",
    "    dep_time_col=\"dep_time\",\n",
    "    arr_time_col=\"arr_time\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a time-expanded graph G = (V, A) from a timetable dataframe.\n",
    "    \n",
    "    Returns:\n",
    "        V         : sorted list of nodes (station, time)\n",
    "        A         : dict {arc_id: {\"kind\", \"from_node\", \"to_node\", \"demand\"}}\n",
    "        incoming  : dict {node: [arc_id, ...]}\n",
    "        outgoing  : dict {node: [arc_id, ...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Copy & ensure types ---\n",
    "    df = arc_df.copy()\n",
    "\n",
    "    # Make sure times are numeric (floats/ints)\n",
    "    df[dep_time_col] = df[dep_time_col].astype(float)\n",
    "    df[arr_time_col] = df[arr_time_col].astype(float)\n",
    "\n",
    "    # Station labels as strings (just in case)\n",
    "    df[stop_from_col] = df[stop_from_col].astype(str)\n",
    "    df[stop_to_col]   = df[stop_to_col].astype(str)\n",
    "\n",
    "    # --- 2. Build stage nodes (station, time) ---\n",
    "    df[\"from_node\"] = list(zip(df[stop_from_col], df[dep_time_col]))\n",
    "    df[\"to_node\"]   = list(zip(df[stop_to_col],   df[arr_time_col]))\n",
    "\n",
    "    V = sorted(set(df[\"from_node\"].tolist()) | set(df[\"to_node\"].tolist()))\n",
    "\n",
    "    # --- 3. Initialize arc dict ---\n",
    "    A = {}\n",
    "    arc_id_counter = 0\n",
    "\n",
    "    def new_arc_id(prefix):\n",
    "        nonlocal arc_id_counter\n",
    "        arc_id = f\"{prefix}_{arc_id_counter}\"\n",
    "        arc_id_counter += 1\n",
    "        return arc_id\n",
    "\n",
    "    # --- 4. Stage arcs (from the rows themselves) ---\n",
    "    for idx, row in df.iterrows():\n",
    "        arc_id = new_arc_id(\"stage\")\n",
    "        A[arc_id] = {\n",
    "            \"kind\":      \"stage\",\n",
    "            \"from_node\": row[\"from_node\"],\n",
    "            \"to_node\":   row[\"to_node\"],\n",
    "            # your demand column name:\n",
    "            \"demand\":    float(row.get(\"avg_ridership\", 0.0)),\n",
    "            \"row_idx\":   int(idx),  # optional: link back to original row\n",
    "        }\n",
    "\n",
    "    # --- 5. Build per-station time sets ---\n",
    "    times_by_station = {}\n",
    "    for (station, t) in V:\n",
    "        times_by_station.setdefault(station, set()).add(t)\n",
    "\n",
    "    # --- 6. Wait arcs (within each station between consecutive times) ---\n",
    "    for st, times in times_by_station.items():\n",
    "        sorted_times = sorted(times)\n",
    "\n",
    "        # If more than one event at this station, connect consecutive events\n",
    "        if len(sorted_times) >= 2:\n",
    "            for t1, t2 in zip(sorted_times[:-1], sorted_times[1:]):\n",
    "                from_node = (st, t1)\n",
    "                to_node   = (st, t2)\n",
    "                arc_id = new_arc_id(\"wait\")\n",
    "                A[arc_id] = {\n",
    "                    \"kind\":      \"wait\",\n",
    "                    \"from_node\": from_node,\n",
    "                    \"to_node\":   to_node,\n",
    "                    \"demand\":    0.0,\n",
    "                }\n",
    "        else:\n",
    "            # Only one time at this station.\n",
    "            # Add a self-loop wait arc so the node has in & out degree ≥ 1,\n",
    "            # even if there are no stage arcs here.\n",
    "            t = sorted_times[0]\n",
    "            from_node = (st, t)\n",
    "            to_node   = (st, t)\n",
    "            arc_id = new_arc_id(\"wait_single\")\n",
    "            A[arc_id] = {\n",
    "                \"kind\":      \"wait\",\n",
    "                \"from_node\": from_node,\n",
    "                \"to_node\":   to_node,\n",
    "                \"demand\":    0.0,\n",
    "            }\n",
    "\n",
    "    # --- 7. Overnight arcs (wrap last time → first time at each station) ---\n",
    "    # This enforces \"circulation over the day\" per station.\n",
    "    for st, times in times_by_station.items():\n",
    "        sorted_times = sorted(times)\n",
    "        first_t = sorted_times[0]\n",
    "        last_t  = sorted_times[-1]\n",
    "        from_node = (st, last_t)\n",
    "        to_node   = (st, first_t)\n",
    "\n",
    "        arc_id = new_arc_id(\"overnight\")\n",
    "        A[arc_id] = {\n",
    "            \"kind\":      \"overnight\",\n",
    "            \"from_node\": from_node,\n",
    "            \"to_node\":   to_node,\n",
    "            \"demand\":    0.0,\n",
    "        }\n",
    "\n",
    "    # --- 8. Build adjacency lists for flow conservation ---\n",
    "    incoming = {v: [] for v in V}\n",
    "    outgoing = {v: [] for v in V}\n",
    "\n",
    "    for arc_id, a_data in A.items():\n",
    "        u = a_data[\"from_node\"]\n",
    "        v = a_data[\"to_node\"]\n",
    "        # Ensure all nodes are present\n",
    "        if u not in outgoing:\n",
    "            outgoing[u] = []\n",
    "            if u not in incoming:\n",
    "                incoming[u] = []\n",
    "            V.append(u)\n",
    "        if v not in incoming:\n",
    "            incoming[v] = []\n",
    "            if v not in outgoing:\n",
    "                outgoing[v] = []\n",
    "            V.append(v)\n",
    "        outgoing[u].append(arc_id)\n",
    "        incoming[v].append(arc_id)\n",
    "\n",
    "    # Sort V again in case we appended any new nodes\n",
    "    V = sorted(set(V))\n",
    "\n",
    "    # --- 9. Optional: quick sanity checks ---\n",
    "    # Nodes with no in or out arcs (should be none)\n",
    "    bad_nodes = [\n",
    "        v for v in V\n",
    "        if len(incoming[v]) == 0 or len(outgoing[v]) == 0\n",
    "    ]\n",
    "    if bad_nodes:\n",
    "        print(\"WARNING: some nodes have zero in-degree or out-degree:\")\n",
    "        for v in bad_nodes:\n",
    "            print(\n",
    "                f\"  node={v} in_deg={len(incoming[v])} \"\n",
    "                f\"out_deg={len(outgoing[v])}\"\n",
    "            )\n",
    "\n",
    "    return V, A, incoming, outgoing\n",
    "# Build graph from your timetable\n",
    "V, A, incoming, outgoing = build_time_expanded_graph(arc_df)\n",
    "\n",
    "seats_per_unit = 162\n",
    "max_units_per_train = 9\n",
    "\n",
    "model = gp.Model(\"Bart_Fleet_Optimization\")\n",
    "\n",
    "# One integer var f[a] per arc\n",
    "f = model.addVars(A.keys(), vtype=GRB.INTEGER, lb=0, name=\"f\")\n",
    "\n",
    "# Flow conservation at all nodes\n",
    "for v in V:\n",
    "    model.addConstr(\n",
    "        gp.quicksum(f[a] for a in incoming[v]) ==\n",
    "        gp.quicksum(f[a] for a in outgoing[v]),\n",
    "        name=f\"flow_{v}\"\n",
    "    )\n",
    "\n",
    "# Demand + capacity on stage arcs\n",
    "for arc_id, a_data in A.items():\n",
    "    if a_data[\"kind\"] == \"stage\":\n",
    "        # capacity must cover demand\n",
    "        model.addConstr(\n",
    "            seats_per_unit * f[arc_id] >= a_data[\"demand\"],\n",
    "            name=f\"demand_{arc_id}\"\n",
    "        )\n",
    "        # optional max coupling constraint\n",
    "        model.addConstr(\n",
    "            f[arc_id] <= max_units_per_train,\n",
    "            name=f\"max_units_{arc_id}\"\n",
    "        )\n",
    "\n",
    "# Objective: minimize number of units in overnight arcs\n",
    "overnight_arcs = [arc_id for arc_id, a_data in A.items()\n",
    "                  if a_data[\"kind\"] == \"overnight\"]\n",
    "\n",
    "model.setObjective(\n",
    "    gp.quicksum(f[a] for a in overnight_arcs),\n",
    "    GRB.MINIMIZE\n",
    ")\n",
    "\n",
    "model.optimize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
